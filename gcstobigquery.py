# -*- coding: utf-8 -*-
"""GCStoBIGQUERY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qDFhCRb8NhtOfhY8pY3d414Q-FFzCFSw
"""

!pip install apache_beam
#create a driver program, which uses classes from beam SDK, this defines our pipeline.
#create a pipeline object.
!pip install apache_beam
from google.cloud import storage
# [START pubsub_to_gcs]
import argparse
from datetime import datetime
import logging
import random
import apache_beam as beam
from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys
from apache_beam.options.pipeline_options import PipelineOptions

# we are going to read data from external sources--GCS bucket-- Creating Pcollections
#applying PTransformations for transforming data

# Transformations
class TransformData(beam.DoFn):
    def process(self, element, count=beam.Create(0)):
        
        message, send_time = element.split(',')
        if message=='Hello':

        
        # we need to replace the message by its count
        
        yield { 'timestamp':timestamp,'count':count}

# Wrting to BigQuery
# Parsing method to prepare data to make writable to BigQuery (dictionary format)
def parse_method(element):
    message, count, timestamp = element
    row = {'message': message, 'count': count, 'timestamp': timestamp}
    return row

#Run the pipeline
def run(argv=None):
    # Create the pipeline options
    options = PipelineOptions(argv)
    p = beam.Pipeline(options=options)

    # Read the data from GCS using the beam.io.ReadFromText
    data = p | 'Read Data' >> beam.io.ReadFromText('gs://your-bucket/input-data/*')
    
    # Apply transformations, we are going to perfrom PTransfroms on the previously created PCollection
    transformed_data = data | 'Transform Data' >> beam.ParDo(TransformData()).with_state_ful(beam.state.CombineFnState())
    | 'Tuple to BigQuery Row' >> beam.Map(lambda s: parse_method(s))
    # Write the data to BigQuery
    transformed_data | 'Write to BigQuery' >> 
                beam.io.WriteToBigQuery(
                    
                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
                )
            
    
    result = p.run()
    result.wait_until_finish()

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',
                        dest='input',
                        default='gs://your_bucket/input',
                        help='Input file to process.'
    )
  
    parser.add_argument('--output',
                        dest='output',
                        default='your_project:your_dataset.your_table',
                        help='Output BigQuery table to write results to.'
        
    )
 
    known_args, pipeline_args = parser.parse_known_args()

    run(
        known_args.input,
        known_args.output,

        pipeline_args,
    )