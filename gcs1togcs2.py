# -*- coding: utf-8 -*-
"""GCS1TOGCS2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10c_pbEqxW38YvhZ0GqZPykutirc5j28M
"""

!pip install apache_beam

import apache_beam as apache_beam
import argparse
from datetime import datetime
import logging
import random
import re
from google.cloud import storage
from apache_beam import DoFn, GroupByKey, io, ParDo, Pipeline, PTransform, WindowInto, WithKeys
from apache_beam.options.pipeline_options import PipelineOptions





def format_date(element):
    #retrieving 2 days previous data and creating a new folder for that day.
    current_date = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
    return f'gs://bucket2/output/{current_date}/output_files-{current_date}.txt'

def run(argv=None):
    options = PipelineOptions(argv)
    with beam.Pipeline(options=options) as pipeline:
        pcoll = (pipeline
                 | 'Read data from bucket 1' >> beam.io.ReadFromText('gs://bucket1/input_files/{current_date}*.txt', skip_header_lines=1)
                 | 'Flatten' >> beam.Flatten()
                 | 'Partition' >> beam.Partition(lambda x, num_partitions: hash(x) % num_partitions, 2)
                 | 'Write to bucket 2' >> beam.io.WriteToText(beam.Map(format_date), file_name_suffix='.txt')
                 | 'Delete input files' >> beam.Map(lambda x: os.system(f'gsutil -m rm gs://bucket1/input_files/input_files/{current_date}.txt'))
                 )

if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input_path",
        help="The path of the input GCS files including the prefix.",
    )

    parser.add_argument(
        "--output_path",
        help="The path of the output GCS folder including the prefix.",
    )

    known_args, pipeline_args = parser.parse_known_args()

    run(
        known_args.input_topic,
        known_args.output_path,

        pipeline_args,
    )

